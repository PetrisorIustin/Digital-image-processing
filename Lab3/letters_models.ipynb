{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import list_datasets, extract_training_samples, extract_test_samples\n",
    "list_datasets()\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = extract_training_samples('letters')[0]\n",
    "labels = extract_training_samples('letters')[1]\n",
    "\n",
    "test_images = extract_test_samples('letters')[0]\n",
    "test_labels = extract_test_samples('letters')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.setflags(write=1)\n",
    "test_labels.setflags(write=1)\n",
    "X_train\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] in (1,5,9,15,21):\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] in (1,5,9,15,21):\n",
    "        test_labels[i] = 1\n",
    "    else:\n",
    "        test_labels[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:10]\n",
    "X_train = images.reshape(images.shape[0], 28, 28, 1)\n",
    "X_test = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM\n",
    "from keras import backend as K\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-79b64ade18ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#add model layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.build((None, 28, 28, 1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 18433     \n",
      "=================================================================\n",
      "Total params: 37,537\n",
      "Trainable params: 37,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 124800 samples, validate on 20800 samples\n",
      "Epoch 1/3\n",
      "124800/124800 [==============================] - ETA: 4:23 - loss: 2.8397 - acc: 0.753 - ETA: 4:13 - loss: 6.7854 - acc: 0.482 - ETA: 4:07 - loss: 5.1131 - acc: 0.587 - ETA: 4:01 - loss: 4.4934 - acc: 0.645 - ETA: 3:57 - loss: 4.2569 - acc: 0.674 - ETA: 3:52 - loss: 4.0792 - acc: 0.694 - ETA: 3:47 - loss: 3.9022 - acc: 0.713 - ETA: 3:44 - loss: 3.7615 - acc: 0.726 - ETA: 3:41 - loss: 3.6817 - acc: 0.735 - ETA: 3:39 - loss: 3.5955 - acc: 0.743 - ETA: 3:37 - loss: 3.5313 - acc: 0.748 - ETA: 3:35 - loss: 3.4642 - acc: 0.752 - ETA: 3:33 - loss: 3.3904 - acc: 0.755 - ETA: 3:31 - loss: 3.2727 - acc: 0.761 - ETA: 3:29 - loss: 3.1391 - acc: 0.764 - ETA: 3:27 - loss: 2.9927 - acc: 0.767 - ETA: 3:25 - loss: 2.8600 - acc: 0.765 - ETA: 3:23 - loss: 2.7395 - acc: 0.762 - ETA: 3:21 - loss: 2.6293 - acc: 0.758 - ETA: 3:19 - loss: 2.5246 - acc: 0.757 - ETA: 3:17 - loss: 2.4271 - acc: 0.758 - ETA: 3:16 - loss: 2.3374 - acc: 0.761 - ETA: 3:14 - loss: 2.2550 - acc: 0.764 - ETA: 3:12 - loss: 2.1782 - acc: 0.767 - ETA: 3:11 - loss: 2.1081 - acc: 0.768 - ETA: 3:09 - loss: 2.0418 - acc: 0.771 - ETA: 3:07 - loss: 1.9814 - acc: 0.773 - ETA: 3:05 - loss: 1.9262 - acc: 0.775 - ETA: 3:04 - loss: 1.8732 - acc: 0.777 - ETA: 3:02 - loss: 1.8242 - acc: 0.779 - ETA: 3:00 - loss: 1.7793 - acc: 0.780 - ETA: 2:58 - loss: 1.7360 - acc: 0.782 - ETA: 2:56 - loss: 1.6950 - acc: 0.784 - ETA: 2:54 - loss: 1.6558 - acc: 0.786 - ETA: 2:52 - loss: 1.6189 - acc: 0.787 - ETA: 2:50 - loss: 1.5837 - acc: 0.789 - ETA: 2:48 - loss: 1.5510 - acc: 0.791 - ETA: 2:47 - loss: 1.5197 - acc: 0.792 - ETA: 2:45 - loss: 1.4898 - acc: 0.794 - ETA: 2:43 - loss: 1.4608 - acc: 0.795 - ETA: 2:41 - loss: 1.4324 - acc: 0.797 - ETA: 2:39 - loss: 1.4064 - acc: 0.799 - ETA: 2:37 - loss: 1.3815 - acc: 0.800 - ETA: 2:35 - loss: 1.3569 - acc: 0.802 - ETA: 2:33 - loss: 1.3339 - acc: 0.804 - ETA: 2:31 - loss: 1.3125 - acc: 0.805 - ETA: 2:29 - loss: 1.2914 - acc: 0.806 - ETA: 2:27 - loss: 1.2710 - acc: 0.807 - ETA: 2:25 - loss: 1.2517 - acc: 0.809 - ETA: 2:23 - loss: 1.2333 - acc: 0.810 - ETA: 2:21 - loss: 1.2158 - acc: 0.811 - ETA: 2:19 - loss: 1.1981 - acc: 0.812 - ETA: 2:17 - loss: 1.1811 - acc: 0.813 - ETA: 2:16 - loss: 1.1647 - acc: 0.815 - ETA: 2:14 - loss: 1.1488 - acc: 0.816 - ETA: 2:12 - loss: 1.1331 - acc: 0.817 - ETA: 2:10 - loss: 1.1185 - acc: 0.819 - ETA: 2:08 - loss: 1.1041 - acc: 0.820 - ETA: 2:06 - loss: 1.0903 - acc: 0.821 - ETA: 2:04 - loss: 1.0770 - acc: 0.822 - ETA: 2:02 - loss: 1.0643 - acc: 0.823 - ETA: 2:00 - loss: 1.0520 - acc: 0.824 - ETA: 1:58 - loss: 1.0408 - acc: 0.825 - ETA: 1:56 - loss: 1.0294 - acc: 0.826 - ETA: 1:54 - loss: 1.0182 - acc: 0.827 - ETA: 1:53 - loss: 1.0077 - acc: 0.827 - ETA: 1:51 - loss: 0.9972 - acc: 0.828 - ETA: 1:49 - loss: 0.9867 - acc: 0.829 - ETA: 1:47 - loss: 0.9768 - acc: 0.829 - ETA: 1:45 - loss: 0.9667 - acc: 0.830 - ETA: 1:43 - loss: 0.9570 - acc: 0.831 - ETA: 1:41 - loss: 0.9486 - acc: 0.831 - ETA: 1:39 - loss: 0.9392 - acc: 0.832 - ETA: 1:37 - loss: 0.9305 - acc: 0.833 - ETA: 1:35 - loss: 0.9218 - acc: 0.834 - ETA: 1:33 - loss: 0.9133 - acc: 0.835 - ETA: 1:31 - loss: 0.9046 - acc: 0.835 - ETA: 1:29 - loss: 0.8968 - acc: 0.836 - ETA: 1:27 - loss: 0.8894 - acc: 0.837 - ETA: 1:25 - loss: 0.8819 - acc: 0.837 - ETA: 1:23 - loss: 0.8744 - acc: 0.838 - ETA: 1:21 - loss: 0.8668 - acc: 0.839 - ETA: 1:20 - loss: 0.8598 - acc: 0.839 - ETA: 1:18 - loss: 0.8527 - acc: 0.840 - ETA: 1:16 - loss: 0.8461 - acc: 0.840 - ETA: 1:14 - loss: 0.8397 - acc: 0.841 - ETA: 1:11 - loss: 0.8333 - acc: 0.841 - ETA: 1:09 - loss: 0.8272 - acc: 0.841 - ETA: 1:07 - loss: 0.8212 - acc: 0.842 - ETA: 1:05 - loss: 0.8153 - acc: 0.842 - ETA: 1:03 - loss: 0.8096 - acc: 0.843 - ETA: 1:01 - loss: 0.8036 - acc: 0.843 - ETA: 59s - loss: 0.7976 - acc: 0.844 - ETA: 57s - loss: 0.7919 - acc: 0.84 - ETA: 56s - loss: 0.7867 - acc: 0.84 - ETA: 54s - loss: 0.7815 - acc: 0.84 - ETA: 52s - loss: 0.7763 - acc: 0.84 - ETA: 50s - loss: 0.7711 - acc: 0.84 - ETA: 48s - loss: 0.7659 - acc: 0.84 - ETA: 46s - loss: 0.7609 - acc: 0.84 - ETA: 44s - loss: 0.7561 - acc: 0.84 - ETA: 42s - loss: 0.7509 - acc: 0.84 - ETA: 39s - loss: 0.7460 - acc: 0.84 - ETA: 37s - loss: 0.7415 - acc: 0.84 - ETA: 35s - loss: 0.7369 - acc: 0.84 - ETA: 33s - loss: 0.7325 - acc: 0.85 - ETA: 31s - loss: 0.7282 - acc: 0.85 - ETA: 29s - loss: 0.7238 - acc: 0.85 - ETA: 27s - loss: 0.7199 - acc: 0.85 - ETA: 25s - loss: 0.7158 - acc: 0.85 - ETA: 23s - loss: 0.7114 - acc: 0.85 - ETA: 20s - loss: 0.7072 - acc: 0.85 - ETA: 18s - loss: 0.7031 - acc: 0.85 - ETA: 16s - loss: 0.6994 - acc: 0.85 - ETA: 14s - loss: 0.6956 - acc: 0.85 - ETA: 12s - loss: 0.6919 - acc: 0.85 - ETA: 10s - loss: 0.6882 - acc: 0.85 - ETA: 8s - loss: 0.6845 - acc: 0.8552 - ETA: 6s - loss: 0.6813 - acc: 0.855 - ETA: 3s - loss: 0.6779 - acc: 0.855 - ETA: 1s - loss: 0.6745 - acc: 0.856 - 280s 2ms/step - loss: 0.6716 - acc: 0.8564 - val_loss: 0.2570 - val_acc: 0.8966\n",
      "Epoch 2/3\n",
      "124800/124800 [==============================] - ETA: 4:44 - loss: 0.2202 - acc: 0.918 - ETA: 4:42 - loss: 0.2342 - acc: 0.912 - ETA: 4:41 - loss: 0.2377 - acc: 0.910 - ETA: 4:39 - loss: 0.2485 - acc: 0.904 - ETA: 4:37 - loss: 0.2478 - acc: 0.903 - ETA: 4:35 - loss: 0.2484 - acc: 0.903 - ETA: 4:32 - loss: 0.2483 - acc: 0.904 - ETA: 4:29 - loss: 0.2500 - acc: 0.902 - ETA: 4:28 - loss: 0.2482 - acc: 0.903 - ETA: 4:26 - loss: 0.2486 - acc: 0.903 - ETA: 4:24 - loss: 0.2452 - acc: 0.904 - ETA: 4:23 - loss: 0.2456 - acc: 0.904 - ETA: 4:22 - loss: 0.2445 - acc: 0.904 - ETA: 4:21 - loss: 0.2443 - acc: 0.904 - ETA: 4:20 - loss: 0.2435 - acc: 0.904 - ETA: 4:17 - loss: 0.2437 - acc: 0.903 - ETA: 4:14 - loss: 0.2443 - acc: 0.903 - ETA: 4:12 - loss: 0.2451 - acc: 0.902 - ETA: 4:09 - loss: 0.2464 - acc: 0.901 - ETA: 4:07 - loss: 0.2475 - acc: 0.901 - ETA: 4:04 - loss: 0.2473 - acc: 0.901 - ETA: 4:02 - loss: 0.2460 - acc: 0.901 - ETA: 3:59 - loss: 0.2468 - acc: 0.901 - ETA: 3:56 - loss: 0.2472 - acc: 0.901 - ETA: 3:54 - loss: 0.2457 - acc: 0.902 - ETA: 3:51 - loss: 0.2455 - acc: 0.902 - ETA: 3:48 - loss: 0.2459 - acc: 0.902 - ETA: 3:45 - loss: 0.2456 - acc: 0.902 - ETA: 3:42 - loss: 0.2450 - acc: 0.902 - ETA: 3:39 - loss: 0.2451 - acc: 0.902 - ETA: 3:36 - loss: 0.2448 - acc: 0.902 - ETA: 3:33 - loss: 0.2457 - acc: 0.902 - ETA: 3:30 - loss: 0.2452 - acc: 0.902 - ETA: 3:27 - loss: 0.2452 - acc: 0.902 - ETA: 3:25 - loss: 0.2463 - acc: 0.902 - ETA: 3:22 - loss: 0.2461 - acc: 0.902 - ETA: 3:19 - loss: 0.2459 - acc: 0.902 - ETA: 3:17 - loss: 0.2455 - acc: 0.903 - ETA: 3:14 - loss: 0.2451 - acc: 0.903 - ETA: 3:12 - loss: 0.2457 - acc: 0.903 - ETA: 3:09 - loss: 0.2463 - acc: 0.902 - ETA: 3:06 - loss: 0.2458 - acc: 0.902 - ETA: 3:04 - loss: 0.2455 - acc: 0.902 - ETA: 3:01 - loss: 0.2453 - acc: 0.903 - ETA: 2:59 - loss: 0.2452 - acc: 0.903 - ETA: 2:56 - loss: 0.2445 - acc: 0.903 - ETA: 2:54 - loss: 0.2449 - acc: 0.903 - ETA: 2:51 - loss: 0.2447 - acc: 0.903 - ETA: 2:49 - loss: 0.2447 - acc: 0.903 - ETA: 2:46 - loss: 0.2453 - acc: 0.903 - ETA: 2:44 - loss: 0.2457 - acc: 0.903 - ETA: 2:41 - loss: 0.2457 - acc: 0.903 - ETA: 2:39 - loss: 0.2456 - acc: 0.903 - ETA: 2:37 - loss: 0.2459 - acc: 0.903 - ETA: 2:35 - loss: 0.2460 - acc: 0.902 - ETA: 2:32 - loss: 0.2457 - acc: 0.903 - ETA: 2:30 - loss: 0.2457 - acc: 0.903 - ETA: 2:28 - loss: 0.2456 - acc: 0.903 - ETA: 2:25 - loss: 0.2455 - acc: 0.903 - ETA: 2:23 - loss: 0.2456 - acc: 0.902 - ETA: 2:21 - loss: 0.2455 - acc: 0.903 - ETA: 2:18 - loss: 0.2455 - acc: 0.903 - ETA: 2:16 - loss: 0.2454 - acc: 0.903 - ETA: 2:14 - loss: 0.2454 - acc: 0.903 - ETA: 2:12 - loss: 0.2454 - acc: 0.902 - ETA: 2:09 - loss: 0.2450 - acc: 0.903 - ETA: 2:07 - loss: 0.2447 - acc: 0.903 - ETA: 2:05 - loss: 0.2448 - acc: 0.903 - ETA: 2:02 - loss: 0.2444 - acc: 0.903 - ETA: 2:00 - loss: 0.2444 - acc: 0.903 - ETA: 1:58 - loss: 0.2440 - acc: 0.903 - ETA: 1:55 - loss: 0.2437 - acc: 0.903 - ETA: 1:53 - loss: 0.2437 - acc: 0.903 - ETA: 1:51 - loss: 0.2435 - acc: 0.903 - ETA: 1:48 - loss: 0.2431 - acc: 0.903 - ETA: 1:46 - loss: 0.2427 - acc: 0.903 - ETA: 1:44 - loss: 0.2425 - acc: 0.904 - ETA: 1:41 - loss: 0.2426 - acc: 0.904 - ETA: 1:39 - loss: 0.2425 - acc: 0.904 - ETA: 1:37 - loss: 0.2425 - acc: 0.903 - ETA: 1:34 - loss: 0.2423 - acc: 0.904 - ETA: 1:32 - loss: 0.2420 - acc: 0.904 - ETA: 1:30 - loss: 0.2417 - acc: 0.904 - ETA: 1:27 - loss: 0.2415 - acc: 0.904 - ETA: 1:25 - loss: 0.2416 - acc: 0.904 - ETA: 1:23 - loss: 0.2414 - acc: 0.904 - ETA: 1:20 - loss: 0.2412 - acc: 0.904 - ETA: 1:18 - loss: 0.2410 - acc: 0.904 - ETA: 1:16 - loss: 0.2411 - acc: 0.904 - ETA: 1:13 - loss: 0.2408 - acc: 0.904 - ETA: 1:11 - loss: 0.2405 - acc: 0.904 - ETA: 1:09 - loss: 0.2409 - acc: 0.904 - ETA: 1:06 - loss: 0.2404 - acc: 0.904 - ETA: 1:04 - loss: 0.2405 - acc: 0.904 - ETA: 1:02 - loss: 0.2402 - acc: 0.904 - ETA: 59s - loss: 0.2401 - acc: 0.904 - ETA: 57s - loss: 0.2398 - acc: 0.90 - ETA: 55s - loss: 0.2395 - acc: 0.90 - ETA: 52s - loss: 0.2393 - acc: 0.90 - ETA: 50s - loss: 0.2394 - acc: 0.90 - ETA: 48s - loss: 0.2394 - acc: 0.90 - ETA: 45s - loss: 0.2393 - acc: 0.90 - ETA: 43s - loss: 0.2392 - acc: 0.90 - ETA: 41s - loss: 0.2389 - acc: 0.90 - ETA: 39s - loss: 0.2388 - acc: 0.90 - ETA: 36s - loss: 0.2389 - acc: 0.90 - ETA: 34s - loss: 0.2391 - acc: 0.90 - ETA: 32s - loss: 0.2387 - acc: 0.90 - ETA: 29s - loss: 0.2383 - acc: 0.90 - ETA: 27s - loss: 0.2380 - acc: 0.90 - ETA: 25s - loss: 0.2381 - acc: 0.90 - ETA: 22s - loss: 0.2380 - acc: 0.90 - ETA: 20s - loss: 0.2379 - acc: 0.90 - ETA: 18s - loss: 0.2380 - acc: 0.90 - ETA: 15s - loss: 0.2381 - acc: 0.90 - ETA: 13s - loss: 0.2378 - acc: 0.90 - ETA: 11s - loss: 0.2375 - acc: 0.90 - ETA: 8s - loss: 0.2375 - acc: 0.9059 - ETA: 6s - loss: 0.2374 - acc: 0.905 - ETA: 4s - loss: 0.2371 - acc: 0.906 - ETA: 2s - loss: 0.2370 - acc: 0.906 - 300s 2ms/step - loss: 0.2368 - acc: 0.9061 - val_loss: 0.2242 - val_acc: 0.9108\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124800/124800 [==============================] - ETA: 4:37 - loss: 0.2351 - acc: 0.898 - ETA: 4:38 - loss: 0.2310 - acc: 0.904 - ETA: 4:36 - loss: 0.2237 - acc: 0.911 - ETA: 4:36 - loss: 0.2211 - acc: 0.913 - ETA: 4:34 - loss: 0.2187 - acc: 0.915 - ETA: 4:33 - loss: 0.2135 - acc: 0.917 - ETA: 4:30 - loss: 0.2076 - acc: 0.919 - ETA: 4:28 - loss: 0.2091 - acc: 0.919 - ETA: 4:27 - loss: 0.2101 - acc: 0.917 - ETA: 4:26 - loss: 0.2070 - acc: 0.918 - ETA: 4:24 - loss: 0.2090 - acc: 0.917 - ETA: 4:22 - loss: 0.2080 - acc: 0.917 - ETA: 4:19 - loss: 0.2068 - acc: 0.918 - ETA: 4:16 - loss: 0.2083 - acc: 0.917 - ETA: 4:13 - loss: 0.2074 - acc: 0.918 - ETA: 4:12 - loss: 0.2110 - acc: 0.917 - ETA: 4:10 - loss: 0.2115 - acc: 0.916 - ETA: 4:08 - loss: 0.2111 - acc: 0.917 - ETA: 4:04 - loss: 0.2116 - acc: 0.916 - ETA: 4:01 - loss: 0.2117 - acc: 0.917 - ETA: 3:59 - loss: 0.2120 - acc: 0.917 - ETA: 3:55 - loss: 0.2116 - acc: 0.917 - ETA: 3:52 - loss: 0.2126 - acc: 0.917 - ETA: 3:49 - loss: 0.2122 - acc: 0.917 - ETA: 3:46 - loss: 0.2143 - acc: 0.916 - ETA: 3:43 - loss: 0.2146 - acc: 0.916 - ETA: 3:40 - loss: 0.2141 - acc: 0.916 - ETA: 3:37 - loss: 0.2140 - acc: 0.916 - ETA: 3:34 - loss: 0.2139 - acc: 0.916 - ETA: 3:32 - loss: 0.2142 - acc: 0.916 - ETA: 3:29 - loss: 0.2144 - acc: 0.916 - ETA: 3:26 - loss: 0.2149 - acc: 0.915 - ETA: 3:24 - loss: 0.2144 - acc: 0.915 - ETA: 3:21 - loss: 0.2141 - acc: 0.915 - ETA: 3:19 - loss: 0.2136 - acc: 0.916 - ETA: 3:17 - loss: 0.2135 - acc: 0.916 - ETA: 3:14 - loss: 0.2132 - acc: 0.916 - ETA: 3:12 - loss: 0.2132 - acc: 0.916 - ETA: 3:10 - loss: 0.2133 - acc: 0.916 - ETA: 3:08 - loss: 0.2128 - acc: 0.916 - ETA: 3:05 - loss: 0.2140 - acc: 0.916 - ETA: 3:03 - loss: 0.2144 - acc: 0.916 - ETA: 3:01 - loss: 0.2141 - acc: 0.916 - ETA: 2:58 - loss: 0.2149 - acc: 0.916 - ETA: 2:56 - loss: 0.2150 - acc: 0.916 - ETA: 2:53 - loss: 0.2145 - acc: 0.916 - ETA: 2:51 - loss: 0.2140 - acc: 0.916 - ETA: 2:49 - loss: 0.2132 - acc: 0.917 - ETA: 2:46 - loss: 0.2130 - acc: 0.917 - ETA: 2:44 - loss: 0.2127 - acc: 0.917 - ETA: 2:42 - loss: 0.2130 - acc: 0.917 - ETA: 2:39 - loss: 0.2131 - acc: 0.917 - ETA: 2:37 - loss: 0.2134 - acc: 0.916 - ETA: 2:35 - loss: 0.2134 - acc: 0.916 - ETA: 2:33 - loss: 0.2132 - acc: 0.916 - ETA: 2:30 - loss: 0.2131 - acc: 0.917 - ETA: 2:28 - loss: 0.2130 - acc: 0.916 - ETA: 2:26 - loss: 0.2134 - acc: 0.916 - ETA: 2:24 - loss: 0.2132 - acc: 0.916 - ETA: 2:22 - loss: 0.2131 - acc: 0.916 - ETA: 2:19 - loss: 0.2130 - acc: 0.916 - ETA: 2:17 - loss: 0.2126 - acc: 0.917 - ETA: 2:15 - loss: 0.2128 - acc: 0.917 - ETA: 2:13 - loss: 0.2130 - acc: 0.916 - ETA: 2:11 - loss: 0.2125 - acc: 0.917 - ETA: 2:09 - loss: 0.2121 - acc: 0.917 - ETA: 2:07 - loss: 0.2122 - acc: 0.917 - ETA: 2:04 - loss: 0.2122 - acc: 0.917 - ETA: 2:02 - loss: 0.2118 - acc: 0.917 - ETA: 2:00 - loss: 0.2120 - acc: 0.917 - ETA: 1:57 - loss: 0.2117 - acc: 0.917 - ETA: 1:55 - loss: 0.2117 - acc: 0.917 - ETA: 1:53 - loss: 0.2122 - acc: 0.917 - ETA: 1:50 - loss: 0.2121 - acc: 0.917 - ETA: 1:48 - loss: 0.2123 - acc: 0.917 - ETA: 1:46 - loss: 0.2123 - acc: 0.916 - ETA: 1:43 - loss: 0.2123 - acc: 0.916 - ETA: 1:41 - loss: 0.2122 - acc: 0.916 - ETA: 1:39 - loss: 0.2120 - acc: 0.917 - ETA: 1:36 - loss: 0.2120 - acc: 0.916 - ETA: 1:34 - loss: 0.2118 - acc: 0.917 - ETA: 1:32 - loss: 0.2116 - acc: 0.917 - ETA: 1:29 - loss: 0.2116 - acc: 0.917 - ETA: 1:27 - loss: 0.2116 - acc: 0.917 - ETA: 1:25 - loss: 0.2116 - acc: 0.917 - ETA: 1:22 - loss: 0.2114 - acc: 0.917 - ETA: 1:20 - loss: 0.2111 - acc: 0.917 - ETA: 1:18 - loss: 0.2110 - acc: 0.917 - ETA: 1:15 - loss: 0.2110 - acc: 0.917 - ETA: 1:13 - loss: 0.2113 - acc: 0.917 - ETA: 1:11 - loss: 0.2113 - acc: 0.917 - ETA: 1:08 - loss: 0.2113 - acc: 0.917 - ETA: 1:06 - loss: 0.2111 - acc: 0.917 - ETA: 1:04 - loss: 0.2114 - acc: 0.917 - ETA: 1:01 - loss: 0.2114 - acc: 0.917 - ETA: 59s - loss: 0.2114 - acc: 0.917 - ETA: 57s - loss: 0.2115 - acc: 0.91 - ETA: 55s - loss: 0.2112 - acc: 0.91 - ETA: 52s - loss: 0.2111 - acc: 0.91 - ETA: 50s - loss: 0.2109 - acc: 0.91 - ETA: 48s - loss: 0.2108 - acc: 0.91 - ETA: 45s - loss: 0.2107 - acc: 0.91 - ETA: 43s - loss: 0.2108 - acc: 0.91 - ETA: 41s - loss: 0.2108 - acc: 0.91 - ETA: 38s - loss: 0.2110 - acc: 0.91 - ETA: 36s - loss: 0.2109 - acc: 0.91 - ETA: 34s - loss: 0.2110 - acc: 0.91 - ETA: 32s - loss: 0.2109 - acc: 0.91 - ETA: 29s - loss: 0.2106 - acc: 0.91 - ETA: 27s - loss: 0.2105 - acc: 0.91 - ETA: 25s - loss: 0.2104 - acc: 0.91 - ETA: 22s - loss: 0.2102 - acc: 0.91 - ETA: 20s - loss: 0.2103 - acc: 0.91 - ETA: 18s - loss: 0.2104 - acc: 0.91 - ETA: 15s - loss: 0.2105 - acc: 0.91 - ETA: 13s - loss: 0.2103 - acc: 0.91 - ETA: 11s - loss: 0.2104 - acc: 0.91 - ETA: 9s - loss: 0.2102 - acc: 0.9176 - ETA: 6s - loss: 0.2102 - acc: 0.917 - ETA: 4s - loss: 0.2104 - acc: 0.917 - ETA: 2s - loss: 0.2103 - acc: 0.917 - 304s 2ms/step - loss: 0.2105 - acc: 0.9175 - val_loss: 0.2147 - val_acc: 0.9139\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "history = model.fit(X_train,labels,validation_data=(X_test, test_labels), batch_size=1024, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('letter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3963c2e2eb2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
